<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="pt-br">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Texto no R - R Sociais</title>
    <meta name="generator" content="Hugo 0.22" />

    
    <meta name="description" content="Material do Curso de Programação para Ciências Sociais">
    
    <link rel="canonical" href="../tutorial11/">
    
    <meta name="author" content="Curso de Programação para Ciências Sociais">
    

    <meta property="og:url" content="/tutorial11/">
    <meta property="og:title" content="R Sociais">
    <meta property="og:image" content="/logo_r4sc.jpg">
    <meta name="apple-mobile-web-app-title" content="R Sociais">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="../images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="../images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('/fonts/icon.eot');
        src: url('/fonts/icon.eot')
               format('embedded-opentype'),
             url('/fonts/icon.woff')
               format('woff'),
             url('/fonts/icon.ttf')
               format('truetype'),
             url('/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="../stylesheets/application.css">
    <link rel="stylesheet" href="../stylesheets/temporary.css">
    <link rel="stylesheet" href="../stylesheets/palettes.css">
    <link rel="stylesheet" href="../stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="../javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Texto no R
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/R4CS" title="@R4CS on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://github.com/R4CS/site" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="../logo_r4sc.jpg">
        </div>
      
      <div class="name">
        <strong>R Sociais </strong>
        
          <br>
          R4CS/site
        
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      
        <ul class="repo">
          <li class="repo-download">
            <a href="https://github.com/R4CS/site/archive/master.zip" target="_blank" title="Download" data-action="download">
              <i class="icon icon-download"></i> Download
            </a>
          </li>
          <li class="repo-stars">
            <a href="https://github.com/R4CS/site/stargazers" target="_blank" title="Stargazers" data-action="star">
              <i class="icon icon-star"></i> Stars
              <span class="count">&ndash;</span>
            </a>
          </li>
        </ul>
        <hr>
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial 1" href="../tutorial1/">
	
	Tutorial 1
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 2" href="../tutorial2/">
	
	Tutorial 2
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 3" href="../tutorial3/">
	
	Tutorial 3
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 4" href="../tutorial4/">
	
	Tutorial 4
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 5" href="../tutorial5/">
	
	Tutorial 5
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 6" href="../tutorial6/">
	
	Tutorial 6
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 7" href="../tutorial7/">
	
	Tutorial 7
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 8" href="../tutorial8/">
	
	Tutorial 8
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 9" href="../tutorial9/">
	
	Tutorial 9
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 10" href="../tutorial10/">
	
	Tutorial 10
</a>



  
</li>



<li>
  
    



<a class="current" title="Tutorial 11" href="../tutorial11/">
	
	Tutorial 11
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Tutorial 12" href="../tutorial12/">
	
	Tutorial 12
</a>



  
</li>



<li>
  
    



<a  title="Tutorial 13" href="../tutorial13/">
	
	Tutorial 13
</a>



  
</li>


        </ul>
        

        
        <hr>
        <span class="section">The author</span>
        
        <ul>
          

          
          <li>
            <a href="https://github.com/R4CS" target="_blank" title="@R4CS on GitHub">
              @R4CS on GitHub
            </a>
          </li>
          

          
          <li>
            <a href="mailto:programacao.sociais@gmail.com" title="Email of programacao.sociais@gmail.com">
              Contact via email
            </a>
          </li>
          
        </ul>
        
      </div>
    </div>
  </div>
</nav>

	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Texto no R </h1>

			

<h2 id="o-pacote-stringr">O pacote <em>stringr</em></h2>

<h3 id="webscrapping-para-capturar-material-para-o-tutorial">Webscrapping para capturar material para o tutorial</h3>

<p>Nossa primeira tarefa será obter um conjunto de textos com o qual trabalharemos. Classicamente, tutoriais de R sobre strings e mineração de texto utilizam &ldquo;corpus&rdquo; (já veremos o que é isso) de literatura clássica.</p>

<p>Para tornar nosso exemplo mais interessante, vamos utilizar discursos na Câmara dos Deputados. Em particular, vamos raspar todos os discursos da Deputada Luiza Erundina no site da Câmara dos Deputados. Vamos começar carregando os pacotes <em>rvest</em> e <em>stringr</em>:</p>

<pre><code class="language-r">library(rvest)
library(stringr)
</code></pre>

<p>A seguir, vamos salvar em um objeto a página que contém uma tabela com os links para os discursos. Note que quando fazemos pesquisa de discurso na Câmara dos Deputados obtemos apenas 20 discursos por página. Alterando o argumento &ldquo;Pagesize&rdquo; no url consegui obter todos (480) os links em uma página única.</p>

<pre><code class="language-r">url_tabela_discursos &lt;- &quot;http://www.camara.leg.br/internet/sitaqweb/DiscursosDeputado.asp?txOrador=LUIZA+ERUNDINA&amp;Campoordenacao=dtSessao&amp;tipoordenacao=DESC&amp;Pagesize=1000&amp;txUF=SP&quot;
</code></pre>

<p>Vamos capturar os links de cada discurso. Examine a url antes de prosseguir para aprender um pouco mais de webscrapping.</p>

<pre><code class="language-r">url_discursos &lt;- url_tabela_discursos %&gt;%
  read_html() %&gt;%
  html_nodes(xpath = &quot;//table[@class ='tabela-padrao-bootstrap table-bordered']//td/a&quot;) %&gt;%
  html_attr(name = &quot;href&quot;)
</code></pre>

<p>O resultado é um vetor com o conteúdo dos atributos &ldquo;href&rdquo;. Precisamos adicionar o início da url para indicar que estamos navegando no servidor da Câmara e retirar os espaços vazios do url, que não são um problema para um browser com o Firefox, mas é um problema para o R.</p>

<p>Aproveitemos para ver duas funções novas, ambas do pacote <em>stringr</em>. Várias delas, como veremos, são semelhantes a funções de outros pacotes com as quais já trabalhamos. Há, porém, algumas vantagens ao utilizá-las: bugs e comportamentos inesperados corrigidos, uso do operador &ldquo;pipe&rdquo;, nomes intuitivos e sequência de argumentos intuitivos.</p>

<p><em>str_c</em> (aka string concatenar) é uma função semelhante a <em>paste0</em> e serve para concatenar dois pedaços de texto, inclusive quando a operação for entre um texto e um vetor e entre vetores.</p>

<p><em>str_replace_all</em>, por sua vez, substitui no texto um padrão por outro, respectivamente na sequência de argumentos. Seu uso é semelhante à função <em>gsub</em>, mas os argumentos estão em ordem intuitiva. Por exemplo, estamos substituindo espaço por nada nos url:</p>

<pre><code class="language-r">url_discursos &lt;- str_c(&quot;http://www.camara.leg.br/internet/sitaqweb/&quot;, url_discursos)
url_discursos &lt;- str_replace_all(url_discursos, &quot; &quot;, &quot;&quot;)
</code></pre>

<p>Vamos agora passar por todos os urls e obter os discursos. Examine a primeira url do vetor antes de prosseguir para aprender um pouco mais de webscrapping. Gravaremos os discursos em um objeto chamado &ldquo;discursos&rdquo;, e cada posição conterá um discurso.</p>

<pre><code class="language-r">discursos &lt;- c()
for (url_discurso in url_discursos) {

  discurso &lt;- url_discurso %&gt;%
    read_html() %&gt;%
    html_nodes(xpath = &quot;//div[@id  = 'content']//p&quot;) %&gt;%
    html_text()
  
  discursos &lt;- c(discursos, discurso)
  
  Sys.sleep(0.5)
}
</code></pre>

<h3 id="funcionalidades-do-stringr">Funcionalidades do <em>stringr</em></h3>

<p>Qual é o tamanho de cada discurso? Vamos aplicar <em>str_length</em> para descobrir. Seu uso é semelhante ao da função <em>nchar</em>:</p>

<pre><code class="language-r">len_discursos &lt;- str_length(discursos)
len_discursos
</code></pre>

<p>Vamos agora observar quais são os discursos nos quais a deputada menciona &ldquo;Constituição&rdquo;. Para tanto, usamos <em>str_detect</em></p>

<pre><code class="language-r">str_detect(discursos, &quot;Constituição&quot;)
</code></pre>

<p>Poderíamos usar o vetor lógico resultante para gerar um subcojunto dos discursos, apenas com aqueles nos quais a palavra &ldquo;Constituição&rdquo; é mencionada. Mais simples, porém, é utilizara função <em>str_subset</em>, que funciona tal qual <em>str_detect</em>, mas resulta num subconjunto em lugar de um vetor lógico:</p>

<pre><code class="language-r">discursos_constituicao &lt;- str_subset(discursos, &quot;Constituição&quot;)
</code></pre>

<p>Se quisessemos apenas a posição no vetor dos discursos que contêm &ldquo;Constituição&rdquo;, <em>str_which</em> faria o trabalho:</p>

<pre><code class="language-r">str_which(discursos, &quot;Constituição&quot;)
</code></pre>

<p>Voltando ao vetor completo, quantas vezes &ldquo;Constituição&rdquo; é mencionada em cada discursos? Qual é o máximo de menções a &ldquo;Constituição&rdquo; em um único discurso?</p>

<pre><code class="language-r">str_count(discursos, &quot;Constituição&quot;)
max(str_count(discursos, &quot;Constituição&quot;))
</code></pre>

<p>Vamos fazer uma substituição nos discursos. No lugar de &ldquo;Constituição&rdquo; colocaremos a expressão &ldquo;Constituição, aquele pedaço de papel que não vale nada,&rdquo;. Podemos fazer a substituição com <em>str_replace</em> ou com <em>str_replace_all</em>. A diferença entre ambas é que <em>str_replace</em> substitui apenas a primeira ocorrênca encontrada, enquanto <em>str_replace_all</em> substitui todas as ocorrências.</p>

<pre><code class="language-r">str_replace(discursos_constituicao, &quot;Constituição&quot;, &quot;Constituição, aquele pedaço de papel que não vale nada,&quot;)
str_replace_all(discursos_constituicao, &quot;Constituição&quot;, &quot;Constituição, aquele pedaço de papel que não vale nada,&quot;)
</code></pre>

<p>Em vez de substituir, queremos conhecer a posição das ocorrências de &ldquo;Constituição&rdquo;. Com <em>str_locate</em> e <em>str_locate_all</em>, respectivamente para a primeira ocorrência e todas as ocorrências, obtemos a posição de começo e fim do padrão buscado:</p>

<pre><code class="language-r">str_locate(discursos_constituicao, &quot;Constituição&quot;)
str_locate_all(discursos_constituicao, &quot;Constituição&quot;)
</code></pre>

<p>Finalmente, notemos que os discursos começam sempre mais ou menos da mesma forma. Vamos retirar os 100 primeiros caracteres de cada discurso para observá-los. Usamos a função <em>str_sub</em>, semelhante à função <em>substr</em>, para extrair um padaço de uma string:</p>

<pre><code class="language-r">str_sub(discursos, 1, 100)
</code></pre>

<p>As posições para extração de exerto podem ser variáveis. Por exemplo, vamos usar &ldquo;len_discursos&rdquo; que criamos acima para extrair os 50 últimos caracteres de cada discurso:</p>

<pre><code class="language-r">str_sub(discursos, (len_discursos - 50), len_discursos)
</code></pre>

<p>Note que alguns discursos começam e terminam com espaços. Para nos livrarmos deles (apenas daqueles no começo e fim da string), utilizamos <em>str_trim</em>:</p>

<pre><code class="language-r">str_trim(discursos)
</code></pre>

<p>Infelizmente, não há tempo suficiente para entrarmos neste tutorial em um tema extremamante útil: expressões regulares. Expressões regulares, como podemos deduzir pelo nome, são expressões que nos permite localizar &ndash; e, portanto, substituir, extrair, parear, etc &ndash; sequências de caracteres com determinadas caraterísticas - por exemplo, &ldquo;quaisquer caracteres entre parênteses&rdquo;, ou &ldquo;qualquer sequência entre espaços que comece com 3 letras e termine com 4 números&rdquo; (placa de automóvel).</p>

<p>Você pode ler um pouco sobre expressões regulares no R <a href="https://rstudio-pubs-static.s3.amazonaws.com/74603_76cd14d5983f47408fdf0b323550b846.html">aqui</a> se tiver tempo em sala de aula. Com o uso de expressões regulares, outros dois pares de funções são bastante úteis <em>str_extract</em>, <em>str_extract_all</em>, <em>str_match</em> e <em>str_match_all</em>.</p>

<h2 id="nuvem-de-palavras">Nuvem de Palavras</h2>

<p>Com a função <em>wordcloud</em> do pacote de mesmo nome, podemos rapidamente visualizar as palavras discursadas tendo o tamanho como função da frequência (vamos limitar a 50 palavras):</p>

<pre><code class="language-r">library(wordcloud)
wordcloud(discursos, max.words = 50)
</code></pre>

<p>Não muito bonita. Voltaremos a fazer nuvem de palavras depois de aprendermos outra maneiras de trabalharmos com texto como dado no R.</p>

<h2 id="corpus-e-o-pacote-tm">Corpus e o pacote tm</h2>

<p>O pacote mais popular para trabalharmos com texto no R se chama <em>tm</em> (&ldquo;Text Mining&rdquo;). Vamos carregá-lo e passar por algumas funções do pacote para, então, trabalharmos com uma nova classe de objeto: Corpus.</p>

<p>Carregue o pacote.</p>

<pre><code class="language-r">library(tm)
</code></pre>

<p>Uma boa prática ao trabalharmos com texto é transformarmos todas as palavras em minúsculas (exceto, obviamente, quando a diferenciação importar). <em>tolower</em>, função da biblioteca básica do R, cumpre a tarefa e vamos criar um objeto &ldquo;discursos2&rdquo;, que será nossa versão modificada dos discursos.</p>

<pre><code class="language-r">discursos2 &lt;- tolower(discursos)
discursos2[1]
</code></pre>

<p>Pontuação também costuma ser um problema ao trabalharmos com texto. A não ser que nos interesse recortar o texto usando os pontos como marcas, convém aplicarmos a função <em>removePunctuation</em> do pacote <em>tm</em> para retirar a pontuação:</p>

<pre><code class="language-r">discursos2 &lt;- removePunctuation(discursos2)
discursos2[1]
</code></pre>

<p>O mesmo ocorre com números. Se não forem de interesse específico, melhor extraí-los. A função <em>removeNumbers</em> resolve o problema:</p>

<pre><code class="language-r">discursos2 &lt;- removeNumbers(discursos2)
discursos2[1]
</code></pre>

<p>Vamos olhar novamente para a nuvem de palavras, usando agora o nosso objeto de texto transformado:</p>

<pre><code class="language-r">wordcloud(discursos2, max.words = 50)
</code></pre>

<p>Note que as palavras com mais frequência são aquelas de maior ocorrência na língua portuguese. Qual é a utilidade de incluí-las na análise se sabemos que são frequentes?</p>

<p>O pacote <em>tm</em> oferece a função <em>stopwords</em>. Essa função gera um vetor com as palavras mais frequentes da língua indicada:</p>

<pre><code class="language-r">stopwords(&quot;pt&quot;)
</code></pre>

<p>Com a função <em>removeWords</em> podemos excluir as &ldquo;stopwords&rdquo; da língua portuguesa de nosso conjunto de textos:</p>

<pre><code class="language-r">discursos2 &lt;- removeWords(discursos2, stopwords(&quot;pt&quot;))
discursos2[1]
</code></pre>

<p>Vamos aproveitar que já fizemos inúmeras remoções &ndash; pontuação, números e stopwords &ndash; e retirar os espaços excedentes que sobraram no texto:</p>

<pre><code class="language-r">discursos2 &lt;- stripWhitespace(discursos2)
discursos2[1]
</code></pre>

<p>E vamos repetir nossa nuvem de palavras:</p>

<pre><code class="language-r">wordcloud(discursos2, max.words = 50)
</code></pre>

<p>Muito mais interessante, não?</p>

<p>Note, porém, o destaque a &ldquo;presidente&rdquo;. A deputada faz referências ao presidente da Câmara em praticamente todos os seus discursos e isso aumenta demais a frequência desta palavra. O mesmo ocorre com &ldquo;luiza&rdquo; e &ldquo;erundina&rdquo;, já que todas as vezes em que inicia uma fala, seu nome é transcrito.</p>

<p>Podemos, então, incrementar a lista de stopwords com padrões que conhecemos:</p>

<pre><code class="language-r">stopwords_pt &lt;- c(stopwords(&quot;pt&quot;), &quot;presidente&quot;, &quot;é&quot;, &quot;sr&quot;, &quot;sra&quot;, &quot;luiza&quot;, 
                  &quot;erundina&quot;, &quot;oradora&quot;, &quot;revisão&quot;, &quot;sp&quot;, &quot;v.exa&quot;)
</code></pre>

<p>E gerar um novo objeto removendo as novas stopwords:</p>

<pre><code class="language-r">discursos3 &lt;- removeWords(discursos2, stopwords_pt)
wordcloud(discursos3, max.words = 50)
</code></pre>

<p>Com uma imagem, podemos ter alguma ideia dos temas e termos recorrentes da deputada.</p>

<p>Uma funcionalidade do pacote <em>tm</em> não muito bem implementada em português é a &ldquo;stemização de palavras&rdquo;. &ldquo;Word Stem&rdquo;, em linguística, significa extrair de um conjunto de palavras apenas a raiz da palavra ou o denominador comum de várias palavras. Por exemplo, &ldquo;discurso&rdquo;, &ldquo;discursivo&rdquo;, &ldquo;discursar&rdquo; e &ldquo;discussão&rdquo;, &ldquo;stemizadas&rdquo;, deveriam se tornar &ldquo;discus&rdquo;, e poderíamos agrupá-las para fins analíticos. Vamos ver um exemplo em inglês:</p>

<pre><code class="language-r">stemDocument(c(&quot;politics&quot;, &quot;political&quot;, &quot;politically&quot;), language = &quot;english&quot;)
</code></pre>

<p>Vamos ver o resultado da função <em>stemDocument</em> no primeiro discurso:</p>

<pre><code class="language-r">discursos4 &lt;- stemDocument(discursos2, language = &quot;portuguese&quot;)
discursos4[1]
</code></pre>

<p>Hummmm&hellip; meio estranho, não? Mas você pegou o espírito. Vamos seguir em frente</p>

<h3 id="tokenização">Tokenização</h3>

<p>Tokenização de um texto significa a separação em pequenos &ldquo;tokens&rdquo;, que podem ser palavras ou n-grams, que são pequenos conjuntos de palavras. Bigrams, por exemplo, são pares de palavras. Voltaremos a esse tópico adiante e com mais cuidado. Mas vamos aproveitar o objeto tal como está para apresentarmos uma função do pacote <em>stringr</em> que deixamos propositalmente para trás: <em>str_split</em>. Como as palavras estão separadas por espaço, no resultado final será uma lista contendo um vetor de tokens para cada discurso:</p>

<pre><code class="language-r">tokens &lt;- str_split(discursos2, &quot; &quot;)
</code></pre>

<p><em>unlist</em> transforma a lista em um vetor único:</p>

<pre><code class="language-r">unlist(tokens)
</code></pre>

<h3 id="corpus">Corpus</h3>

<p>Corpus, em linguística, é um conjunto de textos, normalmente grande e em formato digital. Um corpus é composto pelo conteúdo dos textos e pelos metadados de cada texto. Na linguagem R, Corpus é também uma classe de objetos do pacote <em>tm</em> e à qual podemos aplicar uma série de funções e transformações.</p>

<p>Vamos ver como criar um Corpus.</p>

<p>Em primeiro lugar, é preciso uma fonte. A fonte pode ser um vetor, um data frame ou um diretório. Vejamos os dois primeiros, começando com o vetor com o qual já estamos trabalhando:</p>

<pre><code class="language-r">discursos_source &lt;- VectorSource(discursos)
</code></pre>

<p>&ldquo;discursos_source&rdquo; é um objeto que apenas indica uma fonte de textos para funções do pacote <em>tm</em>. Para criar um Corpus, utilizados a função <em>VCorpus</em> (volatile corpus, com o qual vamos trabalhar, e que armazena os dados na memória) ou <em>PCorpus</em> (permanent corpus, usada para quando os dados estão em uma base externa ao R).</p>

<pre><code class="language-r">discursos_corpus &lt;- VCorpus(discursos_source)
</code></pre>

<p>Vamos observar o objeto &ldquo;discursos_corpus&rdquo; e sua classe:</p>

<pre><code class="language-r">discursos_corpus
class(discursos_corpus)
</code></pre>

<p>Veja que um VCorpus contém &ldquo;Metadata&rdquo; e &ldquo;Content&rdquo;. Neste caso, não temos nenhum metadata sobre os discursos, mas poderíamos criar. Vamos observar o que há na primeira posição de um VCorpus. (hey, note que um VCorpus é uma lista!)</p>

<pre><code class="language-r">str(discursos_corpus[[1]])
</code></pre>

<p>Em metadata temos diversas variáveis: author, description, id, language, etc. Veja que id está preenchido com a ordem dos discursos e a língua está em inglês, por default. Neste exercícios temos mais controle sobre os metadados, pois capturamos os textos de uma fonte específica, mas seria legal armazenar os metadados de um Corpus para compartilhá-lo ou trabalhar com Corpora (plural de Corpus) mais complexos.</p>

<p>Aliás, metadados são a única boa razão para trabalharmos com Corpus e não com vetores. Guardar informações sobre os textos é fundamental para selecionarmos subconjuntos e produzirmos análise.</p>

<p>Vamos reabrir os dados usando um data frame como fonte. Vamos criar um:</p>

<pre><code class="language-r">discursos_df &lt;- data.frame(id_discurso = 1:length(discursos), 
                          text = discursos,
                           stringsAsFactors = F)
str(discursos_df)
</code></pre>

<p>E repetir o processo, com a diferença que utilizamos <em>DataframeSource</em> para indicar a fonte dos dados:</p>

<pre><code class="language-r">discursos_df_source &lt;- DataframeSource(discursos_df[,2])
discursos_df_corpus &lt;- VCorpus(discursos_df_source)
</code></pre>

<p>Mesma coisa, não?</p>

<p>Ao trabalharmos com Corpus, não aplicamos diretamente as funções do pacote <em>tm</em>. Em vez disso, utilizamos a função <em>tm_map</em>, que aplica uma outra função a todos os elementos do Corpus. Esse uso lembra as funções do pacote <em>purrr</em> e da família <em>apply</em>, caso você tenha lido sobre elas no livro R for Data Science ou alhures. Observe a remoção de pontuação com <em>removePunctuation</em>:</p>

<pre><code class="language-r">discursos_corpus &lt;- tm_map(discursos_corpus, removePunctuation)
</code></pre>

<p>A aplicação de qualquer função do pacote <em>tm</em> segue este procedimento. Quando a função não pertence ao pacote <em>tm</em>, porém, precisamos &ldquo;passá-la&rdquo; dentro da função <em>content_transformer</em>:</p>

<pre><code class="language-r">discursos_corpus &lt;- tm_map(discursos_corpus, content_transformer(tolower))
</code></pre>

<p>Se você criar uma função para alteração de um texto, você deve utilizar <em>content_transformer</em> também.</p>

<p>Mais dois exemplos, com <em>removeNumbers</em> e <em>removeWords</em>:</p>

<pre><code class="language-r">discursos_corpus &lt;- tm_map(discursos_corpus, removeNumbers)
discursos_corpus &lt;- tm_map(discursos_corpus, removeWords, stopwords(&quot;pt&quot;))
</code></pre>

<p>Porque tanto trabalho? Para trabalharmos com Corpus, que tem a vantagem de armazenar os metadados, em vez de um vetor.</p>

<p>Para poupar seu trabalho, você pode &ldquo;envelopar&rdquo; todas as transformações que quiser produzir em um Corpus em uma função:</p>

<pre><code class="language-r">limpa_corpus &lt;- function(corpus){
  
  corpus &lt;- tm_map(corpus, removePunctuation)
  corpus &lt;- tm_map(corpus, content_transformer(tolower))
  corpus &lt;- tm_map(corpus, removeNumbers)
  corpus &lt;- tm_map(corpus, removeWords, stopwords(&quot;pt&quot;))

  corpus
}
</code></pre>

<p>E aplicar a função aos Corpora com os quais estiver trabalhando:</p>

<pre><code class="language-r">discursos_corpus &lt;- limpa_corpus(discursos_corpus)
</code></pre>

<h3 id="matriz-documento-termo">Matriz Documento-Termo</h3>

<p>O principal uso do pacote <em>tm</em> é gerar uma matriz de documentos-termos (&ldquo;dtm&rdquo;). Basicamente, essa matriz tem cada documento na linha e cada termo na coluna. O conteúdo da célula é a frequência do termo em cada documento.</p>

<pre><code class="language-r">dtm_discursos &lt;- DocumentTermMatrix(discursos_corpus)
</code></pre>

<p>Veja um fragmento da &ldquo;dtm&rdquo; que criamos (documentos 101 a 105 e termos 996 a 1000):</p>

<pre><code class="language-r">as.matrix(dtm_discursos[101:105, 996:1000])
</code></pre>

<p>Se quisermos rapidamente olhar os termos com frequência maior do que, digamos, 500:</p>

<pre><code class="language-r">findFreqTerms(dtm_discursos, 500)
</code></pre>

<p>Há uma série de usos para a classe Corpus do pacote <em>tm</em> para mineração de texto. Não vamos explorá-los e você pode buscar sozinh@. Vamos adotar agora uma abordagem que não envolve a criação de um Corpus.</p>

<h2 id="uma-abordagem-tidy-para-texto">Uma abordagem &ldquo;tidy&rdquo; para texto</h2>

<p>Corpora são os objetos clássicos para processamento de linguagem natural. No R, porém, há uma tendência a deixar tudo &ldquo;tidy&rdquo;. Vamos ver uma abordagem &ldquo;tidy&rdquo;, ou seja, com data frames no padrão do <em>tidyverse</em>, para texto.</p>

<p>Vamos fazer uma rápida introdução, mas recomendo fortemente a leitura do livro <a href="http://tidytextmining.com/">Text Mininig with R</a>, disponível o formato &ldquo;bookdown&rdquo;.</p>

<p>Comecemos carregando os seguintes pacotes:</p>

<pre><code class="language-r">library(tidytext)
library(dplyr)
library(ggplot2)
library(tidyr)
</code></pre>

<p>Vamos recriar o data frame com os discursos:</p>

<pre><code class="language-r">discursos_df &lt;- data_frame(id_discurso = 1:length(discursos), 
                          text = discursos)
glimpse(discursos_df)
</code></pre>

<h3 id="tokens">Tokens</h3>

<p>A primeira função interessante do pacote <em>tidytext</em> é justamente a tokenização de um texto:</p>

<pre><code class="language-r">discursos_token &lt;- discursos_df %&gt;%
  unnest_tokens(word, text)
glimpse(discursos_token)
</code></pre>

<p>Note que a variável _id_discurso, criada por nós, é mantida. &ldquo;text&rdquo;, porém, se torna &ldquo;words&rdquo;, na exata sequência do texto. Veja que o formato de um &ldquo;tidytext&rdquo; é completamnte diferente de um Corpus.</p>

<p>Como excluir stopwords nessa abordagem? Precisamos de um data frame com stopwords. Vamos recriar um vetor stopwords_pt, que é a versão ampliada das stopwords disponíveis no R, e criar um data frame com tal vetor:</p>

<pre><code class="language-r">stopwords_pt &lt;- c(stopwords(&quot;pt&quot;), &quot;presidente&quot;, &quot;é&quot;, &quot;sr&quot;, &quot;sra&quot;, &quot;luiza&quot;, 
                  &quot;erundina&quot;, &quot;oradora&quot;, &quot;revisão&quot;, &quot;sp&quot;, &quot;v.exa&quot;)
stopwords_pt_df &lt;- data.frame(word = stopwords_pt)
</code></pre>

<p>Com <em>anti_join</em> (lembra dessa função?) mantemos em &ldquo;discursos_token&rdquo; apenas as palavras que não estao em &ldquo;stopwords_pt_df&rdquo;</p>

<pre><code class="language-r">discursos_token &lt;- discursos_token %&gt;%
  anti_join(stopwords_pt_df, by = &quot;word&quot;)
</code></pre>

<p>Para observarmos a frequência de palavras nos discursos, usamos <em>count</em>, do pacote <em>dplyr</em>:</p>

<pre><code class="language-r">discursos_token %&gt;%
  count(word, sort = TRUE)
</code></pre>

<p>Com <em>ggplot</em>, podemos construir um gráfico de barras dos temos mais frequêntes, por exemplo, com frequência maior do que 500. Neste ponto do curso, nada do que estamos fazendo abaixo deve ser novo a você:</p>

<pre><code class="language-r">discursos_token %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 500) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n)) +
    geom_col() +
    xlab(NULL) +
    coord_flip()
</code></pre>

<p>Incorporando a função <em>wordcloud</em> a nossa análise:</p>

<pre><code class="language-r">discursos_token %&gt;%
  count(word, sort = TRUE) %&gt;%
  with(wordcloud(word, n, max.words = 50))
</code></pre>

<p>A abordagem &ldquo;tidy&rdquo; para texto nos mantém no território confortável da manipulação de data frames e, particularmente, me parece mais atrativa do que a abordagem via Corpus para um conjunto grande de casos.</p>

<h3 id="bigrams">Bigrams</h3>

<p>Já produzimos duas vezes a tokenização do texto, sem, no entanto, refletir sobre esse procedimento. Tokens são precisam ser formados por palavras únicas. Se o objetivo for, por exemplo, observar a ocorrência conjunta de termos, convém trabalharmos com bigrams (tokens de 2 palavras) ou ngrams (tokens de n palavras). Vejamos como:</p>

<pre><code class="language-r">discurso_bigrams &lt;- discursos_df %&gt;%
  unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2)
</code></pre>

<p>Note que, ao tokenizar o texto, automaticamente foram excluídas as as pontuações e as palavras foram alteradas para minúscula (use o argumento &ldquo;to_lower = FALSE&rdquo; caso não queira a conversão). Vamos contar os bigrams:</p>

<pre><code class="language-r">discurso_bigrams %&gt;%
  count(bigram, sort = TRUE)
</code></pre>

<p>Como, porém, excluir as stopwords quando elas ocorrem em bigrams? Em primeiro, temos que separar os bigrams e duas palavras, uma em cada coluna:</p>

<pre><code class="language-r">bigrams_separated &lt;- discurso_bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)
</code></pre>

<p>E, a seguir, filter o data frame excluindo as stopwords (note que aproveitamos o vetor &ldquo;stopwords_pt&rdquo;):</p>

<pre><code class="language-r">bigrams_filtered &lt;- bigrams_separated %&gt;%
  filter(!word1 %in% stopwords_pt) %&gt;%
  filter(!word2 %in% stopwords_pt)
</code></pre>

<p>ou, usando <em>anti_join</em>, como anteriormente:</p>

<pre><code class="language-r">bigrams_filtered &lt;- bigrams_separated %&gt;%
  anti_join(stopwords_pt_df, by = c(&quot;word1&quot; = &quot;word&quot;)) %&gt;%
  anti_join(stopwords_pt_df, by = c(&quot;word2&quot; = &quot;word&quot;))
</code></pre>

<p>Produzindo a frequência de bigrams:</p>

<pre><code class="language-r">bigram_counts &lt;- bigrams_filtered %&gt;% 
  count(word1, word2, sort = TRUE)
</code></pre>

<p>Reunindo as palavras do bigram que foram separadas para excluirmos as stopwords:</p>

<pre><code class="language-r">bigrams_united &lt;- bigrams_filtered %&gt;%
  unite(bigram, word1, word2, sep = &quot; &quot;)
</code></pre>

<p>A abordagem &ldquo;tidy&rdquo; traz uma tremenda flexibilidade. Se, por exemplo, quisermos ver com quais palavras a palavra &ldquo;poder&rdquo; é antecedida:</p>

<pre><code class="language-r">bigrams_filtered %&gt;%
  filter(word2 == &quot;poder&quot;) %&gt;%
  count(word1, sort = TRUE)
</code></pre>

<p>Ou precedida:</p>

<pre><code class="language-r">bigrams_filtered %&gt;%
  filter(word1 == &quot;poder&quot;) %&gt;%
  count(word2, sort = TRUE)
</code></pre>

<p>Ou ambos:</p>

<pre><code class="language-r">bf1 &lt;- bigrams_filtered %&gt;%
  filter(word2 == &quot;poder&quot;) %&gt;%
  count(word1, sort = TRUE) %&gt;%
  rename(word = word1)

bf2 &lt;- bigrams_filtered %&gt;%
  filter(word1 == &quot;poder&quot;) %&gt;%
  count(word2, sort = TRUE) %&gt;%
  rename(word = word2)

bind_rows(bf1, bf2) %&gt;%
  arrange(-n)
</code></pre>

<p>Super simples e legal, não?</p>

<h3 id="ngrams">Ngrams</h3>

<p>Repetindo o procedimento para &ldquo;trigrams&rdquo;:</p>

<pre><code class="language-r">discursos_df %&gt;%
  unnest_tokens(trigram, text, token = &quot;ngrams&quot;, n = 3) %&gt;%
  separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep = &quot; &quot;) %&gt;%
  anti_join(stopwords_pt_df, by = c(&quot;word1&quot; = &quot;word&quot;)) %&gt;%
  anti_join(stopwords_pt_df, by = c(&quot;word2&quot; = &quot;word&quot;)) %&gt;%
  anti_join(stopwords_pt_df, by = c(&quot;word3&quot; = &quot;word&quot;)) %&gt;%
  count(word1, word2, word3, sort = TRUE)
</code></pre>

<p>&ldquo;sociedade civil organizada&rdquo; é o &ldquo;trigram&rdquo; mais frequente no discurso da deputada.</p>

<h3 id="redes-de-palavras">Redes de palavras</h3>

<p>Para encerrar, vamos a um dos usos mais interessantes do ngrams: a construção de redes de palavras. Precisaremos de dois novos pacotes, <em>igraph</em> e <em>ggraph</em>. Instale-os se precisar:</p>

<pre><code class="language-r">library(igraph)
library(ggraph)
</code></pre>

<p>Em primeiro lugar, transformaremos nosso data frame em um objeto da classe <em>igraph</em>, do pacote de mesmo nome, usado para a presentação de redes no R:</p>

<pre><code class="language-r">bigram_graph &lt;- bigram_counts %&gt;%
  filter(n &gt; 20) %&gt;%
  graph_from_data_frame()
</code></pre>

<p>A seguir, com o pacote <em>ggraph</em>, faremos o grafo a partir dos bigrams dos discursos da deputada:</p>

<pre><code class="language-r">ggraph(bigram_graph, layout = &quot;fr&quot;) +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
</code></pre>

<p>Note que são formadas pequenas associações entre termos que, par a par, caminham juntos. Novamente, não vamos explorar aspectos analíticos da mineração de texto, mas estas associações são informações de grande interessa a depender dos objetivos da análise.</p>

<h2 id="para-além-do-tutorial">Para além do tutorial</h2>

<p>No tutorial, vimos o básico da preparação de textos para mineração, como organizar um Corpus e criar tokens. Além disso, vimos várias utilidades do pacote <em>stringr</em>, que serve para além da mineração de texto e pode ser útil na organização de bases de dados que contém variáveis &ldquo;character&rdquo;.</p>

<p>Se houver tempo em sala de aula e você quiser se aprofundar no assunto, leia alguns dos capítulos de <a href="http://tidytextmining.com/">Text Mininig with R</a>:</p>

<ul>
<li><p><a href="http://tidytextmining.com/sentiment.html">Capítulo 2 - Análise de Sentimento (com textos em inglês)</a></p></li>

<li><p><a href="http://tidytextmining.com/tfidf.html">Capítulo 3 - Análise de frequência de palavras</a></p></li>

<li><p><a href="http://tidytextmining.com/ngrams.html">Capítulo 4 - Relacionamento entre palavras, n-gramas e correlação</a></p></li>

<li><p><a href="http://tidytextmining.com/topicmodeling.html">Capítulo 6 - Topic Modeling</a></p></li>
</ul>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license &ndash;
				
				Documentation built with
				<a href="https://www.gohugo.io" target="_blank">Hugo</a>
				using the
				<a href="http://github.com/digitalcraftsman/hugo-material-docs" target="_blank">Material</a> theme.
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="../tutorial3/" title="Teste Lógico">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              Teste Lógico
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="../tutorial10/" title="">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = '\/';
      var repo_id  = 'R4CS\/site';
    
    </script>

    <script src="../javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

